2025-06-09 07:20:58
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 34 items / 1 error

==================================== ERRORS ====================================
________________ ERROR collecting reporter/test_migrate_data.py ________________
ImportError while importing test module '/app/reporter/test_migrate_data.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib/python3.10/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
reporter/test_migrate_data.py:10: in <module>
    from reporter.database_manager import ( # Added import
E   ImportError: cannot import name 'get_group_memberships_by_member_id' from 'reporter.database_manager' (/app/reporter/database_manager.py)
=========================== short test summary info ============================
ERROR reporter/test_migrate_data.py
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.47s ===============================


--- New Test Run ---


2025-06-09 07:22:24
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 38 items

reporter/test_migrate_data.py ....                                       [ 10%]
reporter/tests/test_database.py ..                                       [ 15%]
reporter/tests/test_database_manager.py ................................ [100%]

============================== 38 passed in 0.98s ==============================
Test run on Mon Jun  9 08:04:09 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 45 items / 1 error

==================================== ERRORS ====================================
_____________ ERROR collecting reporter/tests/test_migrate_data.py _____________
import file mismatch:
imported module 'test_migrate_data' has this __file__ attribute:
  /app/reporter/test_migrate_data.py
which is not the same as the test file we want to collect:
  /app/reporter/tests/test_migrate_data.py
HINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules
=========================== short test summary info ============================
ERROR reporter/tests/test_migrate_data.py
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.43s ===============================
Test run on Mon Jun  9 08:05:44 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py F                                    [100%]

=================================== FAILURES ===================================
_________________________ test_migration_clears_tables _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f7eeeb6e290>

    def test_migration_clears_tables(monkeypatch):
        """
        Tests if process_gc_data correctly clears the relevant tables
        (members, transactions, plans, and sqlite_sequence entries for these)
        before attempting to process any CSV data.
        """
        test_db_path = ":memory:"

        # Monkeypatch DB_FILE used by process_gc_data and its underlying calls.
        # This needs to target where DB_FILE is *looked up* by the functions in migrate_data.
        # Common places:
        # 1. migrate_data.DB_FILE (if migrate_data.py has 'DB_FILE = ...')
        # 2. migrate_data.database_manager.DB_FILE (if migrate_data.py has 'from reporter import database_manager' and uses database_manager.DB_FILE)
        # 3. database_manager.DB_FILE (if migrate_data directly calls functions in database_manager that use its DB_FILE)
        # For this test, we assume process_gc_data uses a DB_FILE accessible in its module scope,
        # or that it calls database_manager functions that use database_manager.DB_FILE.
        # The most direct way process_gc_data gets DB_FILE is from 'from reporter.database_manager import DB_FILE'.
        # So, we patch it in the module where it's directly used by the tested function or its helpers.
        monkeypatch.setattr("reporter.migrate_data.DB_FILE", test_db_path)
        # Also, if database_manager functions are called by process_gc_data and they use DB_FILE internally:
        monkeypatch.setattr("reporter.database_manager.DB_FILE", test_db_path)


        # Monkeypatch the path to the GC CSV file
        monkeypatch.setattr("reporter.migrate_data.GC_CSV_PATH", DUMMY_CSV_PATH)

        # Setup: Create an in-memory database and schema
        # The create_database function might also use the global DB_FILE, so the patch above should cover it.
        # However, create_database itself takes db_path argument.
        conn = sqlite3.connect(test_db_path)
>       create_database(db_path=test_db_path, conn_override=conn) # Ensure schema is created in our test_db_path
E       TypeError: create_database() got an unexpected keyword argument 'db_path'

reporter/tests/test_migrate_data.py:49: TypeError
=========================== short test summary info ============================
FAILED reporter/tests/test_migrate_data.py::test_migration_clears_tables - Ty...
========================= 1 failed, 41 passed in 1.56s =========================
Test run on Mon Jun  9 08:06:40 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py F                                    [100%]

=================================== FAILURES ===================================
_________________________ test_migration_clears_tables _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f80d25c2b60>

    def test_migration_clears_tables(monkeypatch):
        """
        Tests if process_gc_data correctly clears the relevant tables
        (members, transactions, plans, and sqlite_sequence entries for these)
        before attempting to process any CSV data.
        """
        test_db_path = ":memory:"

        # Monkeypatch DB_FILE used by process_gc_data and its underlying calls.
        # This needs to target where DB_FILE is *looked up* by the functions in migrate_data.
        # Common places:
        # 1. migrate_data.DB_FILE (if migrate_data.py has 'DB_FILE = ...')
        # 2. migrate_data.database_manager.DB_FILE (if migrate_data.py has 'from reporter import database_manager' and uses database_manager.DB_FILE)
        # 3. database_manager.DB_FILE (if migrate_data directly calls functions in database_manager that use its DB_FILE)
        # For this test, we assume process_gc_data uses a DB_FILE accessible in its module scope,
        # or that it calls database_manager functions that use database_manager.DB_FILE.
        # The most direct way process_gc_data gets DB_FILE is from 'from reporter.database_manager import DB_FILE'.
        # So, we patch it in the module where it's directly used by the tested function or its helpers.
        monkeypatch.setattr("reporter.migrate_data.DB_FILE", test_db_path)
        # Also, if database_manager functions are called by process_gc_data and they use DB_FILE internally:
        monkeypatch.setattr("reporter.database_manager.DB_FILE", test_db_path)


        # Monkeypatch the path to the GC CSV file
        monkeypatch.setattr("reporter.migrate_data.GC_CSV_PATH", DUMMY_CSV_PATH)

        # Setup: Create an in-memory database and schema
        # The create_database function might also use the global DB_FILE, so the patch above should cover it.
        # The create_database function takes db_name as its argument.
        # When db_name is ":memory:", create_database itself will use an in-memory db
        # and will return the connection, which we can then use.
        conn = create_database(db_name=test_db_path) # This will create tables on the in-memory connection
        assert conn is not None, "Database connection should be returned for in-memory DB"

        cursor = conn.cursor()

        # Setup: Pre-populate some data
        # Members
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 1", "111", "2023-01-01"))
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 2", "222", "2023-01-02"))
        # Plans
        cursor.execute("INSERT INTO plans (plan_name, duration_days, is_active) VALUES (?, ?, ?)", ("Test Plan 1", 30, True))
        # Transactions
        cursor.execute("INSERT INTO transactions (member_id, transaction_type, plan_id, payment_date, start_date, amount_paid) VALUES (?, ?, ?, ?, ?, ?)",
                       (1, "Group Class", 1, "2023-01-01", "2023-01-01", 100))

        # sqlite_sequence entries (simulating autoincrement behavior)
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("members", 2))
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("plans", 1))
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("transactions", 1))
        conn.commit()

        # Verify pre-population
        assert cursor.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 2
        assert cursor.execute("SELECT COUNT(*) FROM plans").fetchone()[0] == 1
        assert cursor.execute("SELECT COUNT(*) FROM transactions").fetchone()[0] == 1
>       assert cursor.execute("SELECT COUNT(*) FROM sqlite_sequence WHERE name IN ('members', 'plans', 'transactions')").fetchone()[0] == 3
E       assert 6 == 3

reporter/tests/test_migrate_data.py:75: AssertionError
----------------------------- Captured stdout call -----------------------------
Database ':memory:' created and tables ensured.
=========================== short test summary info ============================
FAILED reporter/tests/test_migrate_data.py::test_migration_clears_tables - as...
========================= 1 failed, 41 passed in 1.34s =========================
Test run on Mon Jun  9 08:07:51 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py E                                    [100%]

==================================== ERRORS ====================================
________________ ERROR at setup of test_migration_clears_tables ________________

obj = <module 'reporter.database' from '/app/reporter/database.py'>
name = 'DB_FILE', ann = 'reporter.database'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
>           obj = getattr(obj, name)
E           AttributeError: module 'reporter.database' has no attribute 'DB_FILE'

/usr/local/lib/python3.10/dist-packages/_pytest/monkeypatch.py:90: AttributeError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fa5736c6aa0>

    @pytest.fixture
    def migrate_test_db(monkeypatch):
        """Fixture to set up a temporary file-based database for migration tests."""
        os.makedirs(TEST_DB_DIR_MIGRATE, exist_ok=True)

        # Monkeypatch DB_FILE before create_database and process_gc_data are called.
        # This ensures that all references to DB_FILE within the scope of reporter.migrate_data
        # and reporter.database_manager (if it's imported and used by migrate_data) point to our test DB.
        monkeypatch.setattr("reporter.migrate_data.DB_FILE", TEST_DB_FILE_MIGRATE)
        monkeypatch.setattr("reporter.database_manager.DB_FILE", TEST_DB_FILE_MIGRATE) # Critical for consistency
>       monkeypatch.setattr("reporter.database.DB_FILE", TEST_DB_FILE_MIGRATE) # If database.py uses its own DB_FILE constant

reporter/tests/test_migrate_data.py:29:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/usr/local/lib/python3.10/dist-packages/_pytest/monkeypatch.py:104: in derive_importpath
    annotated_getattr(target, attr, ann=module)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <module 'reporter.database' from '/app/reporter/database.py'>
name = 'DB_FILE', ann = 'reporter.database'

    def annotated_getattr(obj: object, name: str, ann: str) -> object:
        try:
            obj = getattr(obj, name)
        except AttributeError as e:
>           raise AttributeError(
                f"{type(obj).__name__!r} object at {ann} has no attribute {name!r}"
            ) from e
E           AttributeError: 'module' object at reporter.database has no attribute 'DB_FILE'

/usr/local/lib/python3.10/dist-packages/_pytest/monkeypatch.py:92: AttributeError
=========================== short test summary info ============================
ERROR reporter/tests/test_migrate_data.py::test_migration_clears_tables - Att...
========================= 41 passed, 1 error in 1.05s ==========================
Test run on Mon Jun  9 08:08:28 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py F                                    [100%]

=================================== FAILURES ===================================
_________________________ test_migration_clears_tables _________________________

migrate_test_db = 'reporter/tests/test_data_migrate/test_migrate_data.db'

    def test_migration_clears_tables(migrate_test_db): # Uses the fixture
        """
        Tests if process_gc_data correctly clears the relevant tables using a file-based test DB.
        """
        test_db_path = migrate_test_db # Get the path from the fixture

        # Connect to the test DB to pre-populate data
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()

        # Setup: Pre-populate some data
        # (The schema is already created by the fixture's call to create_database)
        # Members
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 1", "111", "2023-01-01"))
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 2", "222", "2023-01-02"))
        # Plans
        cursor.execute("INSERT INTO plans (plan_name, duration_days, is_active) VALUES (?, ?, ?)", ("Test Plan 1", 30, True))
        # Transactions
        cursor.execute("INSERT INTO transactions (member_id, transaction_type, plan_id, payment_date, start_date, amount_paid) VALUES (?, ?, ?, ?, ?, ?)",
                       (1, "Group Class", 1, "2023-01-01", "2023-01-01", 100))

        # sqlite_sequence entries (simulating autoincrement behavior)
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("members", 2))
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("plans", 1))
        cursor.execute("INSERT INTO sqlite_sequence (name, seq) VALUES (?, ?)", ("transactions", 1))
        conn.commit()

        # Verify pre-population
        assert cursor.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 2
        assert cursor.execute("SELECT COUNT(*) FROM plans").fetchone()[0] == 1
        assert cursor.execute("SELECT COUNT(*) FROM transactions").fetchone()[0] == 1
        # After inserting into autoincrement tables, sqlite_sequence should have entries.
        # The exact count might include other system sequences or if other tables used AUTOINCREMENT.
        # We are interested in these specific three.
>       assert cursor.execute("SELECT COUNT(*) FROM sqlite_sequence WHERE name IN ('members', 'plans', 'transactions')").fetchone()[0] == 3, \
            "sqlite_sequence should have entries for members, plans, and transactions after population."
E       AssertionError: sqlite_sequence should have entries for members, plans, and transactions after population.
E       assert 6 == 3

reporter/tests/test_migrate_data.py:82: AssertionError
---------------------------- Captured stdout setup -----------------------------
Database 'reporter/tests/test_data_migrate/test_migrate_data.db' created and tables ensured.
=========================== short test summary info ============================
FAILED reporter/tests/test_migrate_data.py::test_migration_clears_tables - As...
========================= 1 failed, 41 passed in 1.41s =========================
Test run on Mon Jun  9 08:09:16 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py F                                    [100%]

=================================== FAILURES ===================================
_________________________ test_migration_clears_tables _________________________

migrate_test_db = 'reporter/tests/test_data_migrate/test_migrate_data.db'

    def test_migration_clears_tables(migrate_test_db): # Uses the fixture
        """
        Tests if process_gc_data correctly clears the relevant tables using a file-based test DB.
        """
        test_db_path = migrate_test_db # Get the path from the fixture

        # Connect to the test DB to pre-populate data
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()

        # Setup: Pre-populate some data
        # (The schema is already created by the fixture's call to create_database)
        # Members
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 1", "111", "2023-01-01"))
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 2", "222", "2023-01-02"))
        # Plans
        cursor.execute("INSERT INTO plans (plan_name, duration_days, is_active) VALUES (?, ?, ?)", ("Test Plan 1", 30, True))
        # Transactions
        cursor.execute("INSERT INTO transactions (member_id, transaction_type, plan_id, payment_date, start_date, amount_paid) VALUES (?, ?, ?, ?, ?, ?)",
                       (1, "Group Class", 1, "2023-01-01", "2023-01-01", 100))
        conn.commit()

        # Verify pre-population
        # sqlite_sequence is managed by SQLite automatically for tables with AUTOINCREMENT.
        # After inserting into members, plans, and transactions, there should be one entry for each.
        assert cursor.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 2
        assert cursor.execute("SELECT COUNT(*) FROM plans").fetchone()[0] == 1
        assert cursor.execute("SELECT COUNT(*) FROM transactions").fetchone()[0] == 1
        # After inserting into autoincrement tables, sqlite_sequence should have entries.
        # The exact count might include other system sequences or if other tables used AUTOINCREMENT.
        # We are interested in these specific three.
        assert cursor.execute("SELECT COUNT(*) FROM sqlite_sequence WHERE name IN ('members', 'plans', 'transactions')").fetchone()[0] == 3, \
            "sqlite_sequence should have entries for members, plans, and transactions after population."

        conn.close() # Close connection used for setup before process_gc_data runs with its own connection.

        # Action: Run the migration process
        # process_gc_data will use the DB_FILE monkeypatched by the fixture
        # process_gc_data might print errors if the dummy CSV is not perfectly aligned with its expectations
        # after the deletion step, but the focus of this test is the deletion itself.
        try:
            process_gc_data()
        except Exception as e:
            # If process_gc_data fails due to CSV processing after deletion,
            # we might want to catch it if the test is only about the deletion part.
            # However, a robust test would ensure the dummy CSV allows it to run to completion or mock the CSV processing part.
            print(f"process_gc_data raised an exception: {e}. Checking table clearing status anyway.")


        # Assertions: Check if tables are cleared
>       assert cursor.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 0, "Members table was not cleared"
E       sqlite3.ProgrammingError: Cannot operate on a closed database.

reporter/tests/test_migrate_data.py:98: ProgrammingError
---------------------------- Captured stdout setup -----------------------------
Database 'reporter/tests/test_data_migrate/test_migrate_data.db' created and tables ensured.
----------------------------- Captured stdout call -----------------------------

Processing Group Class data...
Clearing existing data from tables: transactions, members, plans
Data cleared successfully.
Created new member: Dummy User
Created new plan: Test Plan
=========================== short test summary info ============================
FAILED reporter/tests/test_migrate_data.py::test_migration_clears_tables - sq...
========================= 1 failed, 41 passed in 1.02s =========================
Test run on Mon Jun  9 08:10:20 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py F                                    [100%]

=================================== FAILURES ===================================
_________________________ test_migration_clears_tables _________________________

migrate_test_db = 'reporter/tests/test_data_migrate/test_migrate_data.db'

    def test_migration_clears_tables(migrate_test_db): # Uses the fixture
        """
        Tests if process_gc_data correctly clears the relevant tables using a file-based test DB.
        """
        test_db_path = migrate_test_db # Get the path from the fixture

        # Connect to the test DB to pre-populate data
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()

        # Setup: Pre-populate some data
        # (The schema is already created by the fixture's call to create_database)
        # Members
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 1", "111", "2023-01-01"))
        cursor.execute("INSERT INTO members (client_name, phone, join_date) VALUES (?, ?, ?)", ("Test User 2", "222", "2023-01-02"))
        # Plans
        cursor.execute("INSERT INTO plans (plan_name, duration_days, is_active) VALUES (?, ?, ?)", ("Test Plan 1", 30, True))
        # Transactions
        cursor.execute("INSERT INTO transactions (member_id, transaction_type, plan_id, payment_date, start_date, amount_paid) VALUES (?, ?, ?, ?, ?, ?)",
                       (1, "Group Class", 1, "2023-01-01", "2023-01-01", 100))
        conn.commit()

        # Verify pre-population
        # sqlite_sequence is managed by SQLite automatically for tables with AUTOINCREMENT.
        # After inserting into members, plans, and transactions, there should be one entry for each.
        assert cursor.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 2
        assert cursor.execute("SELECT COUNT(*) FROM plans").fetchone()[0] == 1
        assert cursor.execute("SELECT COUNT(*) FROM transactions").fetchone()[0] == 1
        # After inserting into autoincrement tables, sqlite_sequence should have entries.
        # The exact count might include other system sequences or if other tables used AUTOINCREMENT.
        # We are interested in these specific three.
        assert cursor.execute("SELECT COUNT(*) FROM sqlite_sequence WHERE name IN ('members', 'plans', 'transactions')").fetchone()[0] == 3, \
            "sqlite_sequence should have entries for members, plans, and transactions after population."

        conn.close() # Close connection used for setup before process_gc_data runs with its own connection.

        # Action: Run the migration process
        # process_gc_data will use the DB_FILE monkeypatched by the fixture
        # process_gc_data might print errors if the dummy CSV is not perfectly aligned with its expectations
        # after the deletion step, but the focus of this test is the deletion itself.
        try:
            process_gc_data()
        except Exception as e:
            # If process_gc_data fails due to CSV processing after deletion,
            # we might want to catch it if the test is only about the deletion part.
            # However, a robust test would ensure the dummy CSV allows it to run to completion or mock the CSV processing part.
            print(f"process_gc_data raised an exception: {e}. Checking table clearing status anyway.")

        # Assertions: Reconnect to the database to check if tables are cleared
        conn_assert = sqlite3.connect(test_db_path)
        cursor_assert = conn_assert.cursor()

>       assert cursor_assert.execute("SELECT COUNT(*) FROM members").fetchone()[0] == 0, "Members table was not cleared"
E       AssertionError: Members table was not cleared
E       assert 1 == 0

reporter/tests/test_migrate_data.py:100: AssertionError
---------------------------- Captured stdout setup -----------------------------
Database 'reporter/tests/test_data_migrate/test_migrate_data.db' created and tables ensured.
----------------------------- Captured stdout call -----------------------------

Processing Group Class data...
Clearing existing data from tables: transactions, members, plans
Data cleared successfully.
Created new member: Dummy User
Created new plan: Test Plan
=========================== short test summary info ============================
FAILED reporter/tests/test_migrate_data.py::test_migration_clears_tables - As...
========================= 1 failed, 41 passed in 1.15s =========================
Test run on Mon Jun  9 08:11:07 UTC 2025
============================= test session starts ==============================
platform linux -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0
rootdir: /app
plugins: anyio-4.9.0, cov-6.1.1, json-report-1.5.0, metadata-3.1.1
collected 42 items

reporter/tests/test_database.py ..                                       [  4%]
reporter/tests/test_database_manager.py ................................ [ 80%]
.......                                                                  [ 97%]
reporter/tests/test_migrate_data.py .                                    [100%]

============================== 42 passed in 1.04s ==============================
